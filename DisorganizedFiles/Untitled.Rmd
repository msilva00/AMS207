---
title: "Tutorial on Bayesian Statistics. <br> Homework from BDA3"
author: 
 - "Fernando Hoces de la Guardia"
 - "Guided by: Susan Paddock"
output:
  pdf_document: default
  html_document: default
  word_document: default
---
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "all"} } });
</script>
  
  

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = FALSE)
library('boot')
library('dplyr')
library('nleqslv')
library('mvtnorm')
knitr::opts_chunk$set(cache=TRUE)

set.seed(142857)  
n               <-  50
``` 


[Gelman.sol]: http://www.stat.columbia.edu/~gelman/book/solutions3.pdf
 
# Chapter 1

## 1.1  

### 1a:
$$
\begin{aligned}
p(y)  &= \frac{1}{2} \left( p(y| \theta = 1) + p(y| \theta = 2) \right) \nonumber\\
      &= \frac{1}{2} \left( N(y|1,2^{2}) + N(y|2,2^{2}) \right)
\end{aligned}
$$

```{r , echo=TRUE}
domain          <- seq(-7,10,.02)
dens            <- 0.5*dnorm(domain,1,2) + 0.5*dnorm(domain,2,2)
plot (domain, dens, ylim=c(0,1.1*max(dens)),
type="l", xlab="y", ylab="", xaxs="i",
yaxs="i", yaxt="n", bty="n", cex=2)
```

### 1b:
$$
\begin{aligned}
p(\theta = 1 | y = 1 ) &= \frac{p(\theta = 1)p(y = 1| \theta = 1)}{\sum_{i=1}^{2}p(\theta = i)p(y = 1| \theta = i)} \nonumber \\
                      &= \frac{0.5N(1|1,4)}{\sum_{i=1}^{2}0.5N(1|i,4)}
\end{aligned}
$$

```{r }
p.theta.1       <- function(sigma) {
  res1          <- (0.5*dnorm(1,1,sigma)) /(sum(0.5*dnorm(1,c(1,2),sigma)))
  return(res1)
  }
```  

Evaluating the last expression in the respective cumulative distribution function we get:`r p.theta.1(2)`. **Note: even though we are adding "discrete" number of probabilities, we are still in the continuous space (but for $y=1$) and should evaluate the probabilities in the density function.**


### 1c:
```{r, include=FALSE }
sigma           <- 2^(-2:3)
```

**Table 1: Posterior probabilty of $\theta = 1$, af a function of $\sigma$**

| $\sigma$            | $p(\theta = 1 | y = 1 )$ |  
|  -----------:        | :-------------:          |  
| `r sigma[1]`         | `r p.theta.1(sigma[1])`  |  
| `r sigma[2]`         | `r p.theta.1(sigma[2])`  |  
| `r sigma[3]`         | `r p.theta.1(sigma[3])`  |  
| `r sigma[4]`         | `r p.theta.1(sigma[4])`  |  
| `r sigma[5]`         | `r p.theta.1(sigma[5])`  |  
| `r sigma[6]`         | `r p.theta.1(sigma[6])`  |  


### 1.3  **[NEW]** 
 First: compute posterior of probability of having $Xx$ genes knowing that parents have brown eyes and individual has brown eyes: 
 
$$
\begin{aligned}
&p(Xx | \text{Person and parents have brown eyes}) = \\
&\frac{ Pr(Xx|(XX,XX))Pr((XX,XX)) + Pr(Xx|(Xx,XX))Pr((Xx,XX)) + Pr(Xx|(Xx,Xx))Pr((Xx,Xx)) }
{Pr(Brown|(XX,XX))Pr((XX,XX)) + Pr(Brown|(Xx,XX))Pr((Xx,XX)) + Pr(Brown|(Xx,Xx))Pr((Xx,Xx)) } \nonumber  \\
&=       \frac{ 0 \cdot (1-p)^{4} + 1/2 \cdot 4p(1-p)^{3} + 1/2 \cdot 4p(1-p)^{2} }
{ 1 \cdot (1-p)^{4} + 1 \cdot 4p(1-p)^{3} + (1-Pr(Blue|(Xx,Xx))) \cdot 4p(1-p)^{2} } \\
&=       \frac{ 0 \cdot (1-p)^{4} + 1/2 \cdot 4p(1-p)^{3} + 1/2 \cdot 4p(1-p)^{2} }
{ 1 \cdot (1-p)^{4} + 1 \cdot 4p(1-p)^{3} + 3/4 \cdot 4p(1-p)^{2} } \\
&= \frac{2p}{1+2p}
\end{aligned}
$$ 

This posterior ($p(Xx | \text{Person and parents have brown eyes})$) will be the new prior when computing the probability that Judy's is $Xx$ given that all her kids are brown eyed (husband is $Xx$). 
$$
\begin{aligned}
&p(Judy = Xx | \text{all n kids brown eyes (AKBE) + previous info.}) = \nonumber \\
&\frac{P(Judy = Xx) P(AKBE|Judy=Xx) }{P(Judy = Xx) P(AKBE|Judy=Xx) + P(Judy \neq  Xx) P(AKBE|Judy \neq Xx)} \\
&= \frac{ \frac{2p}{1+2p} (\frac{3}{4})^{n} }{ \frac{2p}{1+2p} (\frac{3}{4})^{n} + \frac{1}{1+2p} }
\end{aligned}
$$ 

Now we want to compute the probability that the first grandchildren will have blue eyes. We know that all Judy's children have brown eyes, hence have genes $Xx$ or $XX$. It follows that the grandkids will have blue eyes only if the kids have $Xx$ genes. A kid will have $Xx$ if Judy has $Xx$ and she and her husband provide $Xx$ ($pr=2/3$, remember that the kid has brown eyes) or if she has $XX$ and her husband provide $Xx$ ($pr=1/2$). Hence the probability of any kid to be $Xx$ is:
$$
\begin{aligned}
P(kid = Xx| \text{all info}) &= P(kid = Xx| \text{all info} + Judy = Xx)P(Judy = Xx) + \\  &P(kid = Xx| \text{all info} + Judy \neq Xx)P(Judy \neq Xx) \nonumber \\
&= \left( \frac{2}{3} \right) \frac{ \frac{2p}{1+2p} (\frac{3}{4})^{n} }{ \frac{2p}{1+2p} (\frac{3}{4})^{n} + \frac{1}{1+2p} } + \left( \frac{1}{2} \right) \frac{ \frac{1}{1+2p} (\frac{3}{4})^{n} }{ \frac{2p}{1+2p} (\frac{3}{4})^{n} + \frac{1}{1+2p} }
\end{aligned}
$$ 

Conditional on a kid having $Xx$ the probability of having a grandchild with blue eyes is 0,1/4 and 1/2 if the spouse is $XX$, $Xx$ and $xx$ respectively. Each spouse type has a probability of $(1-p)^{2}, 2p(1-p)$ and $p^2$ respectively. Hence, the probability of a grandkid with blue eyes is:
$$
\begin{aligned}
P(Grandchildren = Blue|\text{all info}) &=  \frac{ \frac{2}{3}\frac{2p}{1+2p} + \frac{1}{2}\frac{1}{1+2p} }{\frac{2p}{1+2p} (\frac{3}{4})^{n} + \frac{1}{1+2p} } \left( \frac{1}{4}2p(1-p) + \frac{1}{2}p^{2} \right) \\ 
&= \frac{ \frac{2}{3}\frac{2p}{1+2p} + \frac{1}{2}\frac{1}{1+2p} }{\frac{2p}{1+2p} (\frac{3}{4})^{n} + \frac{1}{1+2p} } \left( \frac{1}{2}p \right) \nonumber
\end{aligned}
$$
Note: This solution was reverse engineered from [here.][Gelman.sol]. A few additional lines were added.   


## 1.6 **[NEW]**   
The trick here is that the probability of having a fraternal twin birth with two boys is $1/4 \times 1/125$ and a identical twin birth of boys is $1/2 \times 1/300$. Hence the probability that Elvis had an identical twin is $5/11$. 

## 1.7  *Let's Make a Deal*  
Calculate the probability of winning for each box after one of the empty boxes has been revealed and is not a winning box. 

Lets define the following events:  
  * $A:$ The participant chose the right box at the beginning.  
  * $B:$ The host opens a particular box, among the unchosen ones, such that is 
  empty.  
  * $C:$ Among the unchosen boxes the host chooses a empty box.     
  
And let's compute the probabilities of each of this events.   
$$
\begin{aligned}
Pr(A) &= 1/3\\
Pr(C) &= 1/2\\
Pr(B) &= Pr(B|A)Pr(A) + Pr(B|\neg A)Pr(\neg A) = (1/2)*(1/3) + Pr(B|\neg A)*(2/3)\\
      &= 1/6 + 2/3*(Pr(B|\neg A,C)Pr(C) + Pr(B|\neg A,\neg C)Pr(\neg C)) \\
      &= 1/6 + 2/3*(1*(1/2) + 0*(1/2)) = 1/2 
\end{aligned}  
$$
Using Bayes' theorem we have that the probability of choosing the right box from the beginning, conditional on a unchosen box being revealed as a losing one is:

$$ Pr(A|B) = \frac{Pr(A)Pr(B|A)}{Pr(B)} = \frac{(1/3)*(1/2)}{1/2}=\frac{1}{3}$$

The participant's chances are not equal across remaining boxes! She is worst of staying with her original choice (33% probability of wining instead of 50%!).


More generally if there were $n$ boxes in total and $i$ boxes where revealed, we have that the wrong way of updating the probabilities ($1/(n-i)$) and the Bayesian update ($\frac{i+n*(n-1-i)}{n*(n-i)*(n-i-1)}$) differ significantly as $i \rightarrow n$.  For example the following graph plots both probabilities of winning in a contest with `r n` boxes as the host opens $i$ boxes.

```{r , echo=FALSE}
n               <- 50 
open.boxes      <- seq(1,n-1,1)
bayes.updt  = function(n,i) (i+n*(n-1-i))/((n*(n-i)*(n-i-1)))
wrong.updt  = function(n,i) 1/(n-i)
bayes.gain      <- (bayes.updt(n,open.boxes)-wrong.updt(n,open.boxes)) / wrong.updt(n,open.boxes)  
plot(open.boxes,bayes.gain, type="l", ylab="% Gain in prob of winning", col="red", xlab="boxes open", ylim=c(0,1.1), xlim=c(0,n))
abline(v=49)
title(main = "A Dynamic Version of ''Let's Make a Deal'' \n  Percentage Gain in probability of winning by thinking 'Bayesian' " )
```
  Looking at the graph it seems that the advantages of thinking in a Bayesian fashion are certainly parameter-specific. Also notice that the player here chooses a "stubborn" strategy, I suspect that if she changes boxes in a optimal way the improvement in her chances will be slightly less. Maybe that is the reason why we don't think in a Bayesian fashion all the time.
 
 
 
# Chapter 2  

## 2.1  

$$
\begin{aligned}
P(\theta) &= Beta(4,4)  \\
P( y | \theta) &= Bin(y|n,\theta)  \\
\Rightarrow P(\theta|y) &= Beta(4+y,4+(n-y))
\end{aligned}
$$  
The **wrong** way to answer the question would be:  
$$
\begin{aligned}
P(\theta|y<3) &\propto \sum_{i=0}^{2}Beta(4+i,4+(n-i))
\end{aligned}
$$
The **right** way to answer the question would be:  
$$
\begin{aligned}
P( y<3 | \theta) &= \sum_{i=0}^{2}Bin(i|n,\theta)\\
\Rightarrow P(\theta|y) &\propto \sum_{i=0}^{2} {n \choose i} Beta(4+i,4+(n-i))\\  
\end{aligned}
$$
In this case some part of the proportionality constant *does* matter. 

```{r,echo=TRUE}
domain <- seq(0,1,.01)
dens = apply(sapply(0:2,function(x) choose(10,x)*dbeta(domain,4+x,4+10-x)),1,sum)
plot(domain, dens, type="l")
```  

## 2.14   

### 2.14a   
 Deriving the posterior for a normal likelihood with known variance, unknown mean, and using a normal prior.  [Slide 15 here](http://www.people.fas.harvard.edu/~plam/teaching/methods/conjugacy/conjugacy_print.pdf)
  
**Note:** a good reminder of the main conjugacy relationships can be found [here](http://www.johndcook.com/conjugate_prior_diagram.html)

### 2.16a **[NEW]** 

Suppose $y \sim Bin(n,\theta)$, with $\theta \sim Beta(\alpha, \beta)$. Derive marginal distribution of $y$ (unconditional on $\theta$)

$$
\begin{aligned}
P(y = k) &= \int_{O}^{1}  p(y,\theta) d\theta \\
 &= \int_{O}^{1}  p(y|\theta) p(\theta) d\theta \\
 &= \int_{O}^{1}   {n \choose k} \theta^{k} ( 1 - \theta)^{n-k} \times 
 \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \theta^{\alpha - 1} ( 1 - \theta)^{\beta - 1} d\theta \\
 &= {n \choose k} \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \int_{O}^{1}  \theta^{k + \alpha - 1} ( 1 - \theta)^{n - k + \beta - 1} d\theta \\
 &= {n \choose k} \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \int_{O}^{1} \frac{\Gamma(\alpha + k)\Gamma(\beta + n - k)}{\Gamma(\alpha + \beta + n)} \frac{\Gamma(\alpha + \beta + n)}{\Gamma(\alpha + k)\Gamma(\beta + n - k)}   \theta^{k + \alpha - 1} ( 1 - \theta)^{n - k + \beta - 1} d\theta \\
 &= {n \choose k} \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \frac{\Gamma(\alpha + k)\Gamma(\beta + n - k)}{\Gamma(\alpha + \beta + n)}  \int_{O}^{1}  \frac{\Gamma(\alpha + \beta + n)}{\Gamma(\alpha + k)\Gamma(\beta + n - k)}   \theta^{k + \alpha - 1} ( 1 - \theta)^{n - k + \beta - 1} d\theta \\
 &= {n \choose k} \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \frac{\Gamma(\alpha + k)\Gamma(\beta + n - k)}{\Gamma(\alpha + \beta + n)} 
\end{aligned}
$$  

Where the second to last line follows from integrating the density of a $Beta(k + \alpha, n - k + \beta)$   

## 2.19 **[NEW]** Exponential model with conjugate prior distribution:  

### 2.19a 
Prove conyugacy for $y_{i}|\theta \sim Exp(\theta)$ for $i= 1, \dots ,n$ ,  iid, and $\theta \sim Gamma(\alpha, \beta)$. 

$$
\begin{aligned}
p(\theta| \mathbf{y}) &\propto p(\mathbf{y}|\theta)p(\theta) \\
&= p(\theta) \prod_{i=1}^{n} p(y_{i}|\theta) \\
&= \frac{\beta^{\alpha}}{\Gamma(\alpha)} \theta^{\alpha - 1} exp(-\beta \theta) \times \prod_{i=1}^{n} \theta exp(-\theta y_{i}) \\
&= \frac{\beta^{\alpha}}{\Gamma(\alpha)} \theta^{\alpha - 1} exp(-\beta \theta) \times \prod_{i=1}^{n} \theta exp(-\theta y_{i}) \\
&= \frac{\beta^{\alpha}}{\Gamma(\alpha)}  \times  \theta^{\alpha + n - 1} exp(-\theta(\beta + \sum y_{i}) ) \\
&\propto \frac{(\beta + \sum y_{i})^{\alpha + n}}{\Gamma(\alpha + n)} \theta^{\alpha + n - 1} exp(-\theta(\beta + \sum y_{i}) ) \\
&= Gamma(\alpha + n, \beta + \sum y_{i}) 
\end{aligned} 
$$  

### 2.19b   
Show that the equivalent prior specification for the mean $\phi = 1/\theta$, is inverse-gamma.  
    **Solution:** We will use the transformation method. For this purpose we define: $\phi = g(\theta) = 1/\theta$, This implies that $g^{-1}(\phi) = 1/\phi$ and $\frac{d}{d\phi} g^{-1}(\phi) = - 1/\phi^{2}$. Now:
    
$$
\begin{aligned}
f_{\phi}(\phi) &= f_{\theta}\left( g^{-1}(\phi) \right) |\frac{d}{d\phi}g^{-1}(\phi)|\\
&= \frac{\beta^{\alpha}}{\Gamma(\alpha)} \frac{1}{\phi^{(\alpha - 1)}}  exp(-\beta/\phi) \frac{1}{\phi^2} \\
&= \frac{\beta^{\alpha}}{\Gamma(\alpha)} \phi^{- (\alpha + 1)}  exp(-\beta/\phi) \\
&= Inv\text{-}Gamma(\phi)
\end{aligned}
$$    
   
### 2.19c  
The length of life ($y_{i}$) of a ligth bulb is distributed $Exp(\theta)$, with $\theta \sim Gamma(\alpha, \beta)$ and Coefficient of Variation ($\sqrt{Var(\theta)} / E(\theta)$) equal to $0.5$. How many observation do we need to reduce the posterior CV to $0.1$? 

$$
\begin{aligned}
CV(\theta) &= \frac{\sqrt{\alpha}/\beta}{\alpha/\beta} = \frac{1}{\sqrt{\alpha}} = 0.5 \implies  \alpha = 4 \\
CV(\theta|y) &= \frac{\sqrt{4 + n}/( \beta + \sum y_{i} ) } { (4+n)/(\beta + \sum y_{i})} =  \frac{1}{\sqrt{4 + n}} = 0.1\\
\implies n &= 96
\end{aligned}
$$  

### 2.19d   
How would your answer change if the CV refers to $\phi$ instead of $\theta$?  For this excercise we will use the fact that Inverse-Gamma distributions are conjugate with exponentials, where the resulting posterior has parameters $IG(\alpha +n, \beta + \sum y_{i})$. Following the same steps as in (c), but for the mean and variance of the IG, we get $\alpha = 6$. Solving for the CV of the posterior we get $n = 96$.   

# Chapter 3

## 3.7 **[NEW]**  
Show that the likelihood of two indpendent poisson $v \sim Pois(\theta_{v}), b \sim Pois(\theta_{b})$ is the same as the likelihood of a $b|v+b \sim Bin(v+b, \frac{\theta_{b}}{\theta_{b} + \theta_{v}})$. 

 **Solution:** For this proof I follow [Blitztein & Hwang - Ch4 - p166-167][Intro.Prob]: 
 
- First, we get the distribution of $y = v + b$ (conditioning by $b$ and using the law of total probability): 
$$
\begin{aligned}
P(y=k) &= P(v+b=k) = \sum_{j=1}^{k} P(v + b = k|b = j)P(b = j)\\  
&= \sum_{j=1}^{k} P(v = k - j)P(b = j)\\  
&= \sum_{j=1}^{k} \frac{e^{ -\theta_{v} } \theta_{v}^{k - j} }{(k-j)!} \frac{e^{ -\theta_{b} } \theta_{b}^{j} }{j!}\\  
&= \sum_{j=1}^{k} \frac{e^{ -(\theta_{v} + \theta_{b}) } \theta_{v}^{k - j} \theta_{b}^{j}  }{(k-j)!j!} \frac{k!}{k!}\\
&= \frac{e^{ -(\theta_{v} + \theta_{b}) } }{k!}  \sum_{j=1}^{k} {k \choose j} \theta_{v}^{k - j} \theta_{b}^{j}\\
&= \frac{e^{ -(\theta_{v} + \theta_{b}) }  (\theta_{v} + \theta_{b})^k}{k!}  \\
&= Pois(\theta_{v} + \theta_{b})
\end{aligned}
$$  

- Second, we obtain the distribution of $b|v+b=n$:
$$
\begin{aligned}
P(b=k|v+b = n) &= \frac{P(v + b = n|b = k) P(b = k)}{P(v + b = n)} \\
&= \frac{P(v = n - k) P(b = k)}{P(v + b = n)} \quad \text{using previous result:}\\
&= \frac{Pois(n - k|\theta_{v})Pois(k|\theta_{b})}{Pois(n|\theta_{v} + \theta_{b})}\\
\\
&= \frac{ \frac{e^{ -\theta_{v} } \theta_{v}^{n - k} }{(n - k)!}    \frac{e^{ -\theta_{b} } \theta_{b}^{k} }{k!} }{    \frac{e^{ -(\theta_{v}+\theta_{b}) } (\theta_{v}+\theta_{b})^{n} }{n!}  }\\
\\
&= {n \choose k} \frac{\theta_{v}^{n-k} \theta_{b}^{k}}{(\theta_{v}+\theta_{b})^{n}} \\
&= {n \choose k} \left( \frac{ \theta_{b} }{\theta_{v}+\theta_{b}} \right)^{k} \left( \frac{\theta_{v}}{\theta_{v}+\theta_{b}} \right)^{n - k}\\
&= Bin \left( n=b+v, \frac{\theta_{b}}{\theta_{v}+\theta_{b}} \right)
\end{aligned}
$$  

[Intro.Prob]: http://www.amazon.com/gp/product/1466575573/ref=pd_lpo_sbs_dp_ss_2?pf_rd_p=1944687522&pf_rd_s=lpo-top-stripe-1&pf_rd_t=201&pf_rd_i=188652940X&pf_rd_m=ATVPDKIKX0DER&pf_rd_r=0Y2GG9RTCM87QYXDVJQJ
 
# Chapter 5  

## 5.3 Reproducing results of section 5.5

```{r, echo=TRUE}
#Data:
school.id       <- LETTERS[1:8]
effect          <- c(28,8,-3,7,-1,1,18,12)
se.effect       <- c(15,10,16,11,9,11,10,18) 

pool.est        <- sum(effect*se.effect^-2)/sum(se.effect^-2)
pool.var        <- sum(se.effect^-2)^-1
pool.ci         <- c(-1.96,1.96)*pool.var^.5 + pool.est
```


The pooled estimated effect and variance are `r round(pool.est,2)` and `r round(pool.var,2)`, with a 95% CI of [`r round(pool.ci[1],2)`, `r round(pool.ci[2],2)`].   
*Posterior simulation under the hierarchical model*  
Using the identity:  
$$
\begin{aligned}
p(\theta,\mu,\tau|y) = p(\tau|y)p(\mu|\tau,y)p(\theta|\mu,\tau,y) 
\end{aligned}
$$
And the results from BDA in equation 5.17, 5.20 and 5.21 we code the posteriors $p(\theta_{j} | \tau, \mu, y), p(\mu|\tau,y)$ and $p(\tau|y)$. **Important note:** for this excercise we could have follow *exactly* the same steps as the last excercise. Given some properties of the N-N model, we took a differente path and derive an analytic formula for $p(\mu|\tau,y)$ and $p(\tau|y)$ (instead of sampling from the join posterior density).

```{r, echo=TRUE}
# Eqn 5.17 of BDA3
post.theta.j    <- function(mu,tau,j) {
  ( effect[j] / ( se.effect[j]^2 ) + mu / ( tau^2 ) ) /
  ( 1 / ( se.effect[j]^2 ) + 1 / ( tau^2 ) ) 
}

post.v.theta.j  <- function(tau,j) {
  1 / ( 1 / ( se.effect[j]^2 ) + 1 / ( tau^2 ) )
}


# Eqn 5.20 of BDA3
post.mu.hat     <- function(tau) {
  sum( effect * 1 / ( se.effect^2 +tau^2 ) ) / 
  sum( 1 / ( se.effect^2 + tau^2 ) )
}
post.v.mu       <- function(tau) {
  ( sum( 1 / ( se.effect^2 +tau^2 ) ) )^-1
}
# Eqn 5.21 of BDA3
marginal.tau     <- function(tau) {
  hyper.prior(tau) * 
    ( post.v.mu(tau)^.5 ) * 
      prod( ( ( se.effect^2 + tau^2 )^(-.5) ) * 
              exp( - ( ( effect - post.mu.hat( tau ) )^2 ) 
                   / ( 2 * ( se.effect^2 + tau^2 ) ) ) )
}
```  

Define a hyper-prior and draw 200 samples from each distribution (for all 8 schools).

```{r,echo=TRUE}
samps           <- 200 

hyper.prior     <-  function(tau) 1
tau.grid        <-  seq(0.001,30, length=samps)
pdf.tau         <-  sapply(tau.grid,function(x) marginal.tau(x))
pdf.tau         <-  pdf.tau/sum(pdf.tau)

plot(tau.grid,pdf.tau, type="l", 
     main="Figure 5.5 from BDA3", 
     xlab=expression(tau), 
     ylab="Density")
```

The sampling method in BDA3 suggest to apply the inverse method from the posterior of $\tau$. I don't do this for two reasons: (i) I'm not sure the posterior has a closed for solution for its inverse, and (ii) given that I already have the density, I can directly draw from that distribution sampling using the `sample` command (which leads me to think that this command applies the inverse method). 

```{r,echo=TRUE}
# Sampling
s.tau           <- sample(tau.grid,samps,prob=pdf.tau, replace=TRUE)
s.mu            <- sapply(s.tau,function(x) rnorm(1,post.mu.hat(x),(post.v.mu(x))^0.5))
s.theta         <- NULL 
for (j in 1:length(school.id)) {
  s.theta[[j]]         <- sapply(1:samps, 
                            function(x) 
                            rnorm(1,
                                  post.theta.j(s.mu[x],s.tau[x],j),
                                  (post.v.theta.j(s.tau[x],j))^0.5
                                  ) )
  }
```  

**The following figures replicate the figures in pg 122 in BDA. Before doing the plots we need to 'average over $\mu$'**

$$
\begin{aligned}
E(\theta_{j}|\tau,y)   &= E_{\mu}\left[E(\theta_{j}|\tau,y,\mu)|\tau,y\right] \nonumber \\
                       &= E_{\mu}\left[\frac{ \frac{1}{\sigma_{j}^{2}}y_{j}  + \frac{1}{\tau^{2}}\mu }{\frac{1}{\sigma_{j}^{2}}  + \frac{1}{\tau^{2} } }|\tau,y\right] = \frac{ \frac{1}{\sigma_{j}^{2}}y_{j}  + \frac{1}{\tau^{2}}\hat{\mu} }{\frac{1}{\sigma_{j}^{2}}  + \frac{1}{\tau^{2} } }\\
\nonumber \\                       
Var(\theta_{j}|\tau,y) &= E_{\mu}\left[Var(\theta_{j}|\tau,y,\mu)|\tau,y\right] + Var_{\mu}\left[E(\theta_{j}|\tau,y,\mu)|\tau,y\right] \nonumber \\
                      &= \frac{1}{\frac{1}{\sigma_{j}^{2}}  + \frac{1}{\tau^{2} }} + V_{\mu}\left(\frac{\frac{1}{\tau^{2}}}{\frac{1}{\sigma_{j}^{2}}  + \frac{1}{\tau^{2} }}\right)
\end{aligned}
$$

Where $V_{\mu}$ and $\hat{\mu}$ correspond to the expressions defined in Eq 5.20 of BDA3. 
Below is the code and plot of both equations. 


```{r, echo=TRUE}  
post.theta.j.no.mu     <- function(tau,j) post.theta.j( post.mu.hat( tau ), tau, j )  
post.se.theta.j.no.mu  <- function(tau,j) sqrt( 
  ( post.v.theta.j( tau, j ) ) * ( 1 + post.v.mu( tau ) * tau^(-2) ) )

plot( tau.grid, sapply(tau.grid, function(x) post.theta.j.no.mu(x,1) ), 
      type="l", ylim=c(-5,30), 
      xlab="", ylab="")
title(main="Figure 5.6 from BDA3", 
      xlab=expression(tau), 
      ylab="Estimated treatment effect")

for (j in 2:8) { 
  lines( tau.grid, 
         sapply( tau.grid, function(x) post.theta.j.no.mu(x,j) ) )
}

plot( tau.grid, sapply(tau.grid, function(x) post.se.theta.j.no.mu(x,1) ), 
      type="l", ylim=c(0,20),
      xlab="", ylab="")
title(main="Figure 5.7 from BDA3", 
      xlab=expression(tau), 
      ylab="Posterior Standard Deviation")

for (j in 2:8) { 
  lines( tau.grid, 
         sapply( tau.grid, function(x) post.se.theta.j.no.mu(x,j) ) )
}
```


```{r,echo=TRUE}
s.theta         <- matrix(unlist(s.theta), ncol = 8, byrow = FALSE)
s.theta.sort    <- apply(s.theta, 2, sort)
p               <- t( apply( s.theta.sort, 2, function(x) 
            quantile( x, c( .025, .25, .5, .75, .975), type=1 ) ) )
p               <- round(p,3)
#Bug in line below
#knitr::kable(p, caption="Misclassification and Reliability for Different Thresholds")
```  

**Table 5.3 from BDA3: **  
Now we can use the simulated $\theta_s$ to described the estimated effects in each school. 

| School                   |             |             |             |             |             |           
|  -----------:            | :--------:  | :-------:   | :-------:   | :-----:     | :----:      |        
|                          | 2.5%        | 25%         | median      | 75%         | 97.5%       |            
| `r school.id[1]`         | `r p[1,1]`  | `r p[1,2]`  | `r p[1,3]`  | `r p[1,4]`  | `r p[1,5]`  |      
| `r school.id[2]`         | `r p[2,1]`  | `r p[2,2]`  | `r p[2,3]`  | `r p[2,4]`  | `r p[2,5]`  |      
| `r school.id[3]`         | `r p[3,1]`  | `r p[3,2]`  | `r p[3,3]`  | `r p[3,4]`  | `r p[3,5]`  |      
| `r school.id[4]`         | `r p[4,1]`  | `r p[4,2]`  | `r p[4,3]`  | `r p[4,4]`  | `r p[4,5]`  |      
| `r school.id[5]`         | `r p[5,1]`  | `r p[5,2]`  | `r p[5,3]`  | `r p[5,4]`  | `r p[5,5]`  |      
| `r school.id[6]`         | `r p[6,1]`  | `r p[6,2]`  | `r p[6,3]`  | `r p[6,4]`  | `r p[6,5]`  |      
| `r school.id[7]`         | `r p[7,1]`  | `r p[7,2]`  | `r p[7,3]`  | `r p[7,4]`  | `r p[7,5]`  |      
| `r school.id[8]`         | `r p[8,1]`  | `r p[8,2]`  | `r p[8,3]`  | `r p[8,4]`  | `r p[8,5]`  |      

Here we reproduce figure 5.8
```{r,echo=TRUE}  
par(mfrow=c(1,2))  
domain           <- c(-20,60)
hist(s.theta[,1], breaks=10, xlab="Effect in School A", main="", xlim=domain)
hist(apply(s.theta,1,max), breaks=10, xlab="Largest Effect", main="", xlim=domain)
title(main="Figure 5.8 from BDA3")
```  

This last figure ("largest effect") is a good example of one the main advantage of a fully Bayesian hierarchical model: once we have simulated the posterior, we can test all kinds of complicated hypothesis. 

### 5.3a  
 - (i) For each school $j$, the probability that its coaching program is the best of eight:  
(**Important:** do not sort each posterior). 
```{r, echo=TRUE}
aux1            <- apply(s.theta,1,max)
best            <- apply(1*(s.theta==aux1), 2,mean)
```  

**Table 2: Probability that each coaching program is the best among the eight schools**

| School                   | Probability of having the best coaching program|  
|  -----------:            | :-------------:          |  
| `r school.id[1]`         | `r best[1]`  |  
| `r school.id[2]`         | `r best[2]`  |  
| `r school.id[3]`         | `r best[3]`  |  
| `r school.id[4]`         | `r best[4]`  |  
| `r school.id[5]`         | `r best[5]`  |  
| `r school.id[6]`         | `r best[6]`  |  
| `r school.id[7]`         | `r best[7]`  |  
| `r school.id[8]`         | `r best[8]`  |  

 - (ii) For each school $j$, the probability that its coaching program is better than other school $k$:  

```{r, echo=TRUE}
p               <- sapply( 1:8,
                          function(y) sapply( 1:8,
                                             function(x) 
                                               mean( 1 * ( s.theta[,x] > s.theta[,y] ) ) 
                                             ) 
                          )
```  

\pagebreak

**Table 3: Probability that school $j$ (row) has a better program that school $k$ (column)**  

| School $j$/School $k$|             |             |             |             |             |             |             |             |           
|  -----------:        | :--------:  | :-------:   | :-------:   | :-----:     | :----:      | :----:      | :----:      | :----:      |        
|                      |    A        |   B         |    C        |    D        |    E        |    F        |    G        |    H        |            
| `r school.id[1]`     | `r p[1,1]`  | `r p[1,2]`  | `r p[1,3]`  | `r p[1,4]`  | `r p[1,5]`  | `r p[1,6]`  | `r p[1,7]`  | `r p[1,8]`  |      
| `r school.id[2]`     | `r p[2,1]`  | `r p[2,2]`  | `r p[2,3]`  | `r p[2,4]`  | `r p[2,5]`  | `r p[2,6]`  | `r p[2,7]`  | `r p[2,8]`  |      
| `r school.id[3]`     | `r p[3,1]`  | `r p[3,2]`  | `r p[3,3]`  | `r p[3,4]`  | `r p[3,5]`  | `r p[3,6]`  | `r p[3,7]`  | `r p[3,8]`  |      
| `r school.id[4]`     | `r p[4,1]`  | `r p[4,2]`  | `r p[4,3]`  | `r p[4,4]`  | `r p[4,5]`  | `r p[4,6]`  | `r p[4,7]`  | `r p[4,8]`  |      
| `r school.id[5]`     | `r p[5,1]`  | `r p[5,2]`  | `r p[5,3]`  | `r p[5,4]`  | `r p[5,5]`  | `r p[5,6]`  | `r p[5,7]`  | `r p[5,8]`  |      
| `r school.id[6]`     | `r p[6,1]`  | `r p[6,2]`  | `r p[6,3]`  | `r p[6,4]`  | `r p[6,5]`  | `r p[6,6]`  | `r p[6,7]`  | `r p[6,8]`  |      
| `r school.id[7]`     | `r p[7,1]`  | `r p[7,2]`  | `r p[7,3]`  | `r p[7,4]`  | `r p[7,5]`  | `r p[7,6]`  | `r p[7,7]`  | `r p[7,8]`  |      
| `r school.id[8]`     | `r p[8,1]`  | `r p[8,2]`  | `r p[8,3]`  | `r p[8,4]`  | `r p[8,5]`  | `r p[8,6]`  | `r p[8,7]`  | `r p[8,8]`  |    

### 5.3b 
 - (i) Now with $\tau = \infty$ compute for each school $j$, the probability that it has the best coaching program:  
  With $\tau = \infty$ each school posterior effect is independent $\theta_{j} \sim N(y_{y}, \sigma_{j}^{2})$. The probability of a school having the best coaching program is:  
  **Wrong way to do it:**  
$$
\begin{aligned}  
p(\theta_{j}>max_{i\neq j}\{\theta_{i}\}) &= \prod_{i\neq j} p(\theta_{j}>\theta_{i}) \\
                                          &= \prod_{i\neq j} \Phi(\frac{\theta_{j} - \theta_{i}}{\sigma_{i}})  
\end{aligned}
$$
  
  **Right way to do it:**  
$$
\begin{aligned}  
p(\theta_{j}>max_{i\neq j}\{\theta_{i}\}) &= \int \prod_{i\neq j} p(\theta_{j}>\theta_{i}) \phi(\theta_{j}|y_{j},\sigma_{j})d\theta_{j} \\
                                          &= \int \prod_{i\neq j} \Phi\left(\frac{\theta_{j} - \theta_{i}}{\sigma_{i}}\right) \phi(\theta_{j}|y_{j},\sigma_{j})d\theta_{j}                               
\end{aligned}
$$

This integral has to be solved numerically:
```{r,echo=TRUE}  
set.seed(142857)
best            <-  sapply(1:8,
                      function(y) mean( sapply( 1:1000 ,
                        function(x) 
                          prod( pnorm( ( 
                            rnorm( 1 , effect[y] , se.effect[y] ) - effect[-y] ) / 
                              se.effect[-y] ) ) )
                           )
                           )
# Ad-hoc normalization:
best            <- best/sum(best)  
```  

**Table 4: Probability that each coaching program is the best among the eight schools (with $\tau = \infty$)**

| School                   | Probability of having the best coaching program|  
|  -----------:            | :-------------:          |  
| `r school.id[1]`         | `r best[1]`  |  
| `r school.id[2]`         | `r best[2]`  |  
| `r school.id[3]`         | `r best[3]`  |  
| `r school.id[4]`         | `r best[4]`  |  
| `r school.id[5]`         | `r best[5]`  |  
| `r school.id[6]`         | `r best[6]`  |  
| `r school.id[7]`         | `r best[7]`  |  
| `r school.id[8]`         | `r best[8]`  |  

 - (ii) Now with $\tau = \infty$ compute for each school $j$, the probability that its coaching program is the better than other school $k$:  
$$
\begin{aligned}  
p(\theta_{i}>\theta_{j}) &= p\left(-\frac{y_{j} - y_{i}}{\sqrt{\sigma_{i}^{2} + \sigma_{j}^{2}}} > \frac{(\theta_{j}-\theta_{i})- (y_{j} - y_{i})}{\sqrt{\sigma_{i}^{2} + \sigma_{j}^{2}}} \right) \\
                         &= \Phi\left( \frac{y_{i} - y_{j}}{\sqrt{\sigma_{i}^{2} + \sigma_{j}^{2}}}\right)
\end{aligned}
$$  

The following table presents the different values for the expression above:
```{r, echo=TRUE}
p               <- sapply(1:8,function(x) 
                   sapply(1:8,function(y) 
                  pnorm( q = 0, mean = (effect[x] - effect[y]) / sqrt(se.effect[x]^2 + se.effect[y]^2) , 
                  sd = 1 )
                  ) )  
# Force all elementens in the diagonal to zero. 
p               <- p - .5 * diag(8)
```  

**Table 5: Probability that $j$ (row) has a better program that school $k$ (column). With $\tau = \infty$**  

| School $j$/School $k$|             |             |             |             |             |             |             |             |           
|  -----------:        | :--------:  | :-------:   | :-------:   | :-----:     | :----:      | :----:      | :----:      | :----:      |        
|                      |    A        |   B         |    C        |    D        |    E        |    F        |    G        |    H        |            
| `r school.id[1]`     | `r p[1,1]`  | `r p[1,2]`  | `r p[1,3]`  | `r p[1,4]`  | `r p[1,5]`  | `r p[1,6]`  | `r p[1,7]`  | `r p[1,8]`  |      
| `r school.id[2]`     | `r p[2,1]`  | `r p[2,2]`  | `r p[2,3]`  | `r p[2,4]`  | `r p[2,5]`  | `r p[2,6]`  | `r p[2,7]`  | `r p[2,8]`  |      
| `r school.id[3]`     | `r p[3,1]`  | `r p[3,2]`  | `r p[3,3]`  | `r p[3,4]`  | `r p[3,5]`  | `r p[3,6]`  | `r p[3,7]`  | `r p[3,8]`  |      
| `r school.id[4]`     | `r p[4,1]`  | `r p[4,2]`  | `r p[4,3]`  | `r p[4,4]`  | `r p[4,5]`  | `r p[4,6]`  | `r p[4,7]`  | `r p[4,8]`  |      
| `r school.id[5]`     | `r p[5,1]`  | `r p[5,2]`  | `r p[5,3]`  | `r p[5,4]`  | `r p[5,5]`  | `r p[5,6]`  | `r p[5,7]`  | `r p[5,8]`  |      
| `r school.id[6]`     | `r p[6,1]`  | `r p[6,2]`  | `r p[6,3]`  | `r p[6,4]`  | `r p[6,5]`  | `r p[6,6]`  | `r p[6,7]`  | `r p[6,8]`  |      
| `r school.id[7]`     | `r p[7,1]`  | `r p[7,2]`  | `r p[7,3]`  | `r p[7,4]`  | `r p[7,5]`  | `r p[7,6]`  | `r p[7,7]`  | `r p[7,8]`  |      
| `r school.id[8]`     | `r p[8,1]`  | `r p[8,2]`  | `r p[8,3]`  | `r p[8,4]`  | `r p[8,5]`  | `r p[8,6]`  | `r p[8,7]`  | `r p[8,8]`  |    


### 5.3c    
The estimated differences between the closed form solutions (5.3b) and the bayesian analysis (5.3a) is that the latter presents less extreme probability estimates (shrinkage)   
  
### 5.3d   
If $\tau = 0$, then all effects are the same so the probabilities can be 0 or 1 for all schools (all are the largest effect and the smallest at the same time)
  

##5.13 - Bicycles
```{r 5.13.data,echo=TRUE}
#Load data
y                <- c(16, 9  , 10 , 13 , 19 , 20 , 18 , 17 , 35 , 55 )
n                <- c(74, 99 , 58 , 70 , 122, 77 , 104, 129, 308, 119)
```  

### 5.13a  
$y_{i}\sim Bin(\theta_{i},n_{i})$ where $n_{i}$ represents the *total* number of vehicles (bicycles + other vehicles). $\theta_{i}\sim Beta(\alpha,\beta)$ the prior distribution of biking rates for each street. We set a non-informative hyper-prior $p(\alpha,\beta) \propto (\alpha + \beta)^{-5/2}$. This implies that the **joint posterior** distribution has the following (same as in equation 5.6 in BDA3): 

$$
\begin{aligned}
p(\theta,\alpha,\beta|y) &\propto p(\alpha, \beta)p(\theta|\alpha, \beta)p(y|\theta,\alpha, \beta) \nonumber \\ 
p(\theta,\alpha,\beta|y) &\propto p(\alpha, \beta)\prod^{J}_{j=1} \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \theta_{j}^{\alpha - 1} (1 - \theta_{j})^{\beta - 1}\prod^{J}_{j=1} \theta_{j}^{y_{j}} (1 - \theta_{j})^{n_{j}-y_{j}}\label{bic.joint.post1}
\end{aligned}
$$
  
### 5.13b  
Compute the marginal posterior of $\theta$, conditional on $\alpha, \beta$. For the beta-binomial case we have that given the hyper-parameters, each $\theta_{j}$ has a posterior distribution  $Beta(\alpha + y_{j}, \beta +n_{j} - y_{j})$. Assuming exchangeability:
$$
\begin{aligned}
p(\theta|\alpha,\beta,y) &= \prod^{J}_{j=1} \frac{\Gamma(\alpha + \beta +n_{j})}{\Gamma(\alpha+y_{j})\Gamma(\beta+n_{j}-y_{j})} \theta_{j}^{\alpha+y_{j} - 1} (1 - \theta_{j})^{\beta+n_{j}-y_{j} - 1}\label{bic.cond.post.theta1}
\end{aligned}
$$

  Now we compute the posterior marginal of $(\alpha,\beta)$. Given that we do have a closed form solution in step 2, we compute the ratio of (\\ref{bic.joint.post1}) and (\\ref{bic.cond.post.theta1}). 
  
$$
\begin{aligned}
p(\alpha,\beta|y) &\propto \prod^{J}_{j=1} \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \frac{\Gamma(\alpha+y_{j})\Gamma(\beta+n_{j}-y_{j})}{\Gamma(\alpha + \beta +n_{j})} \label{rat.marg.post.phi1}
\end{aligned}
$$

Centering our grid around the methods of moments estimates for $(\alpha_{0}, \beta_{0})$:

$$
\begin{aligned}
\hat{\mu}     &= `r mean(y/n)` = \frac{\hat{\alpha_{0}}}{\hat{\alpha_{0}}+\hat{\beta_{0}}}\\
\hat{\sigma^2} &= `r sd(y/n)^2`   = \frac{\hat{\alpha_{0}}\hat{\beta_{0}}}{(\hat{\alpha_{0}}+\hat{\beta_{0}})^{2}(\hat{\alpha_{0}}+\hat{\beta_{0}}+1)}
\end{aligned}
$$

Solving form $(\hat{\alpha_{0}},\hat{\beta_{0}})$:

```{r,echo=TRUE}   
#Here 'x' represents alpha and beta
dslnex          <- function(x) {
    z           <- numeric(2)
    z[1]        <- x[1]/(x[1]+x[2]) - mean(y/n)
    z[2]        <- x[1]*x[2]/(((x[1]+x[2])^2)*(x[1]+x[2]+1)) - sd(y/n)^2
    z
}

sol1            <- nleqslv(c(1,1), dslnex) 
res1            <- paste("(",round(sol1$x[1],1), ",", round(sol1$x[2],1), ")",sep="")
```

We get: $(\hat{\alpha_{0}},\hat{\beta_{0}}) = `r res1`$. 

We center the grid (approximately) around that initial estimate and expand the grid to cover up to a factor of 4 of each parameter. The result is plotted in the following figure:  

```{r, echo=TRUE}
bic.marg.post.phi <-   function(alpha, beta) {
  post          <-  1
  #notice the censoring in n (the gamma(.) function in R cannot handle large values)
  for (i in 1:length(y)) {
    if (n[i] > 100) n[i] = 100
    post  = post * ( 
      ( ( gamma(alpha + beta) ) / 
        ( gamma(alpha) * gamma(beta) ) ) * 
      ( ( gamma(alpha + y[i] ) * gamma(beta + n[i] - y[i]) ) / 
        ( gamma(alpha + beta + n[i]) ) ) 
      )
  }
  # The hyper prior is defined below
  bic.hyper.prior(alpha,beta) * post
}

bic.hyper.prior <-  function(alpha,beta) 
{
    alpha*beta*(alpha + beta)^(-5/2)
}

v1              <-  seq(log(sol1$x[1]/sol1$x[2])*1.5,
                        log(sol1$x[1]/sol1$x[2])/1.5,length.out =151)
v2              <-  seq(log(sol1$x[1]+sol1$x[2])/1.5,
                        log(sol1$x[1]+sol1$x[2])*1.5,length.out =151)
beta            <-  exp(v2)/(exp(v1)+1)
alpha           <-  exp(v2+v1)/(exp(v1)+1)

post.dens       <-  outer(alpha,beta,function(x1,x2) log(bic.marg.post.phi(x1, x2)) )
post.dens       <-  exp(post.dens - max(post.dens))
post.dens       <-  post.dens/sum(post.dens)

contours        <-  seq(min(post.dens), max(post.dens) , length=10)
contour(v1, v2, post.dens,
        levels=contours, 
        xlab=expression( log(alpha/beta) ), 
        ylab=expression( log(alpha+beta) ), 
        xlim=c( min( v1 ), max( v1 ) ) , 
        ylim=c( min( v2 ), max( v2 ) ), 
        drawlabels=FALSE, 
        main="Contour plot of joint posterior")
```  

Adjust the grid and repeat:

```{r, echo=TRUE}
v1              <-  seq(log(sol1$x[1]/sol1$x[2])*1.5,
                        log(sol1$x[1]/sol1$x[2])/1.5,length.out =151)
v2              <-  seq(log(sol1$x[1]+sol1$x[2])/3,
                        log(sol1$x[1]+sol1$x[2])*1.5,length.out =151)
beta            <-  exp(v2)/(exp(v1)+1)
alpha           <-  exp(v2+v1)/(exp(v1)+1)

post.dens       <-  outer(alpha,beta,function(x1,x2) log(bic.marg.post.phi(x1, x2)) )
post.dens       <-  exp(post.dens - max(post.dens))
post.dens       <-  post.dens/sum(post.dens)

contours        <-  seq(min(post.dens), max(post.dens) , length=10)
contour(v1, v2, post.dens,
        levels=contours, 
        xlab=expression( log(alpha/beta) ), 
        ylab=expression( log(alpha+beta) ), 
        xlim=c( min( v1 ), max( v1 ) ) , 
        ylim=c( min( v2 ), max( v2 ) ), 
        drawlabels=FALSE, 
        main="Contour plot of joint posterior")
```  

Draw samples $(\alpha^{s}, \beta^{s})$ from $p(\alpha,\beta|y)$ (finally!). Here we repeat the procedure used in section 3.(v) of the book replication document.  
        
    
```{r, echo=TRUE}
samps           <-  1000
#Integrate (sum) over all beta to get the marginal of alpha
v1.dens         <-  apply(post.dens ,1, sum)
s.v1            <-  sample(v1, samps, replace=TRUE, prob = v1.dens)

# Select the colum of the joint density corresponding to a specific value of v1 (p(v2|v1))
cond.v2         <-  function(x) 
{
  post.dens[which(v1 == s.v1[x]),]
}
# Sample a value of v2 according the the conditional probatility above
s.v2            <-  sapply(1:samps,function(x) sample(v2,1,replace=TRUE,prob=cond.v2(x)))

# Add a uniform random jitter centered at zero with with equal to the grid spacing. 
# This will make the simulation draws more continuous. Plot the sampled values.  
grid.v1         <-  v1[2] - v1[1]
grid.v2         <-  v2[2] - v2[1]
s.v2            <-  s.v2 + runif(length(s.v2),-grid.v2/2,grid.v2/2)
s.v1            <-  s.v1 + runif(length(s.v1),-grid.v1/2,grid.v1/2)
plot(s.v1, s.v2, 
     xlab=expression( log(alpha/beta)^s ), 
     ylab=expression( log(alpha+beta)^s ), 
     xlim=c( min(v1) , max(v1) ) , 
     ylim=c( min(v2) , max(v2) ), 
     main="Scatter Plot of Sample Draws of log(alpha/beta) and log(alpha+beta)")
```  

By applying the inverse of the transformation we recover the marginal distribution of the original hyper-parameters. 

```{r, echo=TRUE}
s.beta          <-  exp( s.v2 ) / ( exp(s.v1)+1 )
s.alpha         <-  exp( s.v2 + s.v1 ) / ( exp(s.v1)+1 )  
```

### 5.13c  
For each draw of $\phi^{s}$, draw a sample of $\theta$ from $p(\theta|\phi^{s},y)$ 
```{r, echo=TRUE}
theta.dist      <-  sapply(1:10, 
                           function(x) 
                             rbeta(1000, s.alpha+y[x], s.beta + n[x] - y[x])
                           )
theta.dist      <-  apply(theta.dist,2,sort)
plot(0:600/1000, 0:600/1000,  
     type="l", 
     xlab="Observed rate",
     ylab="95% CI and median of posterior")

jitter.x        <-  y/n + runif(length(y),-0.01,0.01)
points(jitter.x, theta.dist[500,])
segments(jitter.x,theta.dist[25,], jitter.x,theta.dist[975,] )
title(main="Posterior Distribution of Bike rates for all 10 streets")
```   

  The estimated proportions are almost the same as the raw proportions (no shrinkage).  
  
### 5.13d  
We generate 1000 draws from a $Beta(\alpha^{s},\beta^{s})$ where the parameters come from the draws obtained above:
    
```{r,echo=TRUE}  
s.theta         <- rbeta(1000, shape1 =s.alpha , shape2 = s.beta)   
CI.num          <- round(s.theta[order(s.theta)][c(25,975)],2)
CI.str          <- paste("(" , CI.num[1] , "," , CI.num[2] , ")")
```

The posterior interval for $\hat{\theta} = `r CI.str`$   

### 5.13e  
If a new street is opening with 100 vehicles per day. The posterior interval predicts with 95% confidence that between `r CI.num[1]*100` and `r CI.num[2]*100`. This CI is not so informative as it covers almost all the possible observed bike rates. 
  
### 5.13f  
The beta assumption might not have been so reasonable as the posterior estimates did not show much shrinkage. 
  
  
##5.14    

### 5.14a  
Set up a model in which the total number of vehicles observed at each location $j$ follows a Poisson distribution with parameter $\theta_{j}$, the 'true' rate of traffic per hour at the location. Assign a gamma population distribution for the parameters $\theta_{j}$ and a non-informative hyper-prior distribution. Write down the joint posterior distribution.   
  
  Now we have that $n_{j} \sim Poi(\lambda =\theta_{j})$ and $\theta_{j} \sim Gamma(\alpha)$. And the joint posterior is:  

$$
\begin{aligned}
p(\theta,\alpha,\beta|y) &\propto p(\alpha, \beta) \times p(\theta|\alpha, \beta) \times p(y|\theta,\alpha, \beta) \nonumber \\ 
p(\theta,\alpha,\beta|y) &\propto 1\times \prod_{j=1}^{10}Gamma(\theta_{j} | \alpha, \beta) \times \prod_{j=1}^{10}Poisson(y_{j}|\theta_{j}) \nonumber \\
&= \prod_{j=1}^{10}\frac{\beta^{\alpha}}{\Gamma(\alpha)}\theta_{j}^{\alpha-1}exp(-\beta \theta) \times \frac{\theta_{j}^{y_{i}}exp(-\theta_{j})}{!y_{j}} \nonumber \\
&\propto \frac{\beta^{n\alpha}}{\Gamma(\alpha)^{n}}exp(-\sum \theta_{j}( 1 + \beta )) \prod_{j=1}^{10} \theta_{j}^{\alpha + y_{j}-1} \label{bic.joint.post2}
\end{aligned}
$$

### 5.14b  
Then compute the marginal posterior of $\theta$, conditional on $\alpha, \beta$. For the gamma-poisson case we have that given the hyper-parameters, each $\theta_{j}$ has a posterior distribution  $Gamma(\alpha + n_{j}, \beta +1)$. Assuming exchangeability:  

$$
\begin{aligned}
p(\theta|\alpha,\beta,y) &\propto \prod_{j=1}^{10}Gamma(\theta_{j} | \alpha +y_{j}, \beta+1) \nonumber \\
&\propto \prod_{j=1}^{10}Gamma(\theta_{j} | \alpha +y_{j}, \beta+1) \nonumber \\
&\propto \prod_{j=1}^{10} \theta_{j}^{\alpha + y_{j} -1}exp(-(\beta+1) \theta_{j})
\label{bic.cond.post.theta2}
\end{aligned}
$$

  Now we compute the posterior marginal of $(\alpha,\beta)$. Given that we do have a closed form solution in step 2, we compute the ratio of (\\ref{bic.joint.post2}) and (\\ref{bic.cond.post.theta2}). 
$$
\begin{aligned}
p(\alpha,\beta|y) &\propto \frac{\beta^{n\alpha}}{\Gamma(\alpha)^{n}} \prod_{i=1}^{n}\frac{\Gamma(\alpha+y_{i})}{(\beta + 1)^{\alpha+y_{i}}}
\label{bic.marg.post.phi}
\end{aligned}
$$

Centering our grid around the methods of moments estimates for $(\alpha_{0}, \beta_{0})$:

$$
\begin{aligned}
\hat{\mu}     &= `r mean(n)`  = \frac{\hat{\alpha_{0}}}{\hat{\beta_{0}}}\\
\hat{\sigma^2} &= `r sd(n)^2` = \frac{\hat{\alpha_{0}}}{\hat{\beta_{0}}^2}
\end{aligned}
$$

Solving for $(\hat{\alpha_{0}},\hat{\beta_{0}})$:

```{r,echo=TRUE}   
#Here 'x' represents alpha and beta
dslnex          <- function(x) {
    z           <- numeric(2)
    z[1]        <- x[1]/(x[2]) - mean(n)
    z[2]        <- x[1]/(x[2]^2) - sd(n)^2
    z
}

sol1            <- nleqslv(c(1,1), dslnex) 
res1            <- paste("(",round(sol1$x[1],1), ",", round(sol1$x[2],2), ")",sep="")
```

We get: $(\hat{\alpha_{0}},\hat{\beta_{0}}) = `r res1`$. 

We center the grid (approximately) around that initial estimate and expand the grid to cover up to a factor of 4 of each parameter. The result is plotted in the following figure:  
 
```{r, echo=TRUE}
bic.marg.post.phi <-   function(alpha, beta) {
  log.post          <-  0
  #notice the censoring in n
  for (i in 1:length(n)) 
  {
    if (n[i] > 100) n[i]  <-  100
    log.post        <-  log.post + log(gamma( alpha+n[i] )) - (alpha+n[i])*log((beta + 1))
  }
  # The hyper prior is defined below
  log(bic.hyper.prior2(alpha,beta)) + 
    log.post + (length(n)*alpha)*log(beta) - 
    length(n)*log(gamma(alpha))
}

bic.hyper.prior2 <-  function(alpha,beta) {
  1
}


alpha           <-  seq(sol1$x[1]/1.5,sol1$x[1]*1.5,length.out =151)
beta            <-  seq(sol1$x[2]/1.5,sol1$x[2]*1.5,length.out =151)

post.dens       <-  outer(alpha,beta,function(x1,x2) bic.marg.post.phi(x1, x2) )
post.dens       <-  exp(post.dens - max(post.dens))
post.dens       <-  post.dens/sum(post.dens)



contours        <- seq(min(post.dens), max(post.dens), length=10)
contour(alpha, beta, post.dens,levels=contours, 
        xlab=expression(alpha), 
        ylab=expression(beta), 
        xlim=c(min(alpha),max(alpha)) , 
        ylim=c(min(beta),max(beta)), 
        drawlabels=FALSE, 
        main="Contour plot of joint posterior")
```  

Adjust the grid and repeat:

```{r, echo=TRUE}  
alpha           <-  seq(sol1$x[1]/1.5,sol1$x[1]*12,length.out =151)
beta            <-  seq(sol1$x[2]/1.5,sol1$x[2]*12,length.out =151)

post.dens       <-  outer(alpha,beta,function(x1,x2) bic.marg.post.phi(x1, x2) )
post.dens       <-  exp(post.dens - max(post.dens))
post.dens       <-  post.dens/sum(post.dens)

contours        <- seq(min(post.dens), max(post.dens) , length=10)
contour(alpha, beta, post.dens,levels=contours, 
        xlab=expression(alpha), 
        ylab=expression(beta), 
        xlim=c(min(alpha),max(alpha)) , 
        ylim=c(min(beta),max(beta)), 
        drawlabels=FALSE, 
        main="Contour plot of joint posterior")
```  

Draw samples $(\alpha^{s}, \beta^{s})$ from $p(\alpha,\beta|y)$. 
    
```{r, echo=TRUE}
samps           <-  1000
alpha.dens      <-  apply(post.dens ,1, sum)
s.alpha         <-  sample(alpha,samps, replace=TRUE, prob = alpha.dens)

#Select the colum of the joint density corresponding to a specific 
#value of v1 (p(beta|alpha))
cond.beta       <-  function(x) {
  post.dens[which(alpha == s.alpha[x]),]
}
#Sample a value of v2 according the the conditional probatility above
s.beta          <-  sapply(1:samps,function(x) 
  sample(beta,1,replace=TRUE,prob=cond.beta(x)))

#Add a uniform random jitter centered at zero with with equal to the 
#grid spacing. This will make the simulation draws more continuous. 
#Plot the sampled values.  
grid.alpha      <-  alpha[2]-alpha[1]
grid.beta       <-  beta[2]-beta[1]
s.beta          <-  s.beta + runif(length(s.beta),-grid.beta/2,grid.beta/2)
s.alpha         <-  s.alpha + runif(length(s.alpha),-grid.alpha/2,grid.alpha/2)
plot(s.alpha, s.beta, 
     xlab=expression(alpha), 
     ylab=expression(beta), 
     xlim=c(min(alpha),max(alpha)) , 
     ylim=c(min(beta),max(beta)), 
     main="Scatter Plot of Sample Draws of alpha and beta")
```  

**Note:** regardless of how much I change the range of $\alpha$ and $\beta$ I don't seem to cover the whole graph.     

### 5.14c  
Given the previous result we can say that the posterior is not integrable.     
  
### 5.14d   
I don't know how to alter it such that it becomes integrable.  

##5.15 **[NEW]**  Meta-analysis
